---
title: "An Essay on Operational Data Management in Organizations"
description: ""
author: "Wilco Kruijer"
publicationDate: "Jan 20 2025"
heroImage: "/placeholder-logo-text-padded.jpg"
---

# An Essay on Operational Data Management in Organizations

## Goals

### Sync vs. Fetch

- Why data should be synced (over fetched)
  - Two-way sync
- Why reactivity
  - Lazy vs. eager
  - Goodbye caches

### Data Ownership

- Why you need a single source of truth
- Why data should be owned by product teams
  - Why can the data team steal your data?
- Why data ownership enables you to ship faster
  - Maintainability
  - (Loose) Coupling
- Why only owners of a product truly know what the data really means

### Data Catalog

- Why data should be catalogued
- Why integrating data makes for better products
- Why an operational data catalog is good for AI
  - Interfaces vs. implementation: easier for humans -> easier for AI

### Misc

- Why event sourcing has not gone mainstream?

  - Events vs Entities

- What should be easier about data (software/product) evolution

Maybes:

- Contract defined by consumer - Reverse contract

Out takes:

- Should your data model by acyclic?
- SQL vs. procedural

Think bigger:

- Open Platform

Ending

- What does it enable?
  - Local-first

## Notes on concluding

- new use cases: local-first

## Description

description : Although the value of data is clear, not everyone within organizations is equally
happy to handle data. To software engineers, managing state is a necessary evil. This article aims
to change the software engineer's perspective by introducing new ways of managing data within
organisations.

Data within organisations is undoubtedly valuable, not everyone sees this value equally however. To
software engineers, data can be a burden.

## Notes

```
[User] --Update Document--> [Relational Database] --Stream Event--> [Kafka] --Calculate Vector--> [Vector Database]
```

## Scrapped

# Write Path vs. Read Path

When dealing with data in relation to building applications, two distinct stages in the lifecycle of
data can be discerned: the write and the read path. The boundary between these phases can be
adjusted to satisfy the requirements of the application. The write path is what happens eagerly
while the read path is evaluated lazily. In a typical application when the user updates a row in the
database, the system will then eagerly update any indexed related to this row. In a more complex
system, the entity that is being updated might be indexed in an external system such as a vector
database. In that case the act of pushing the update over some event streaming solution to the
external system is also part of the eager evaluation [^30].

Fetching is inherently a procedural paradigm: first you say where and how the data is obtained, then
you do something with this data. The fans of functional programming are right, this _is_ worse than
being declarative about obtaining data [^40]. Synchronization allows us to be declarative about
obtaining—and updating—data. This is good because it allows for hiding the implementation (the how).
Consider the vector database example above, using a conventional approach you'd have to manually
publish the data to Kafka, implement a consumer, and again push the data to the vector store.

[^40]: The existence and widespreadness of SQL is more evidence for this fact.

## How a sync engine works

I'll give a short summary of how such an engine works, but the implementation details should be the
focus of a different post. Syncing starts by having clients indicate their current offset in the
global stream of changes. Next, they'd start listening for any changes after that point. There are
practical concerns like the initial sync (the first time the client connects), and partitioning.
These concerns however, are simply implementation details of the sync engine.

## Local-first

Relatively recently, synchronization has seen use in the context of frontend applications [^30].
Software written this way is known as "local-first" software. This has a number of advantages
compared to traditional server/client-model software: instant (optimistic) updates, out of the box
multi-user live collaboration, and offline support. Of course the concept of eager evaluation is not
new to frontend developers. Declarative UI frameworks have existed for years, and React has shown
that the concept of reactivity (declarative + eager) is brilliant to work with.

## Martin Reference

[^20]:
    The ideas discussed in this paragraph are based on Martin Kleppmann's book,
    ["Designing Data-Intensive Applications"](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/).

## Sync conclusion

With a sync engine built on top of existing infrastructure, the burden of transferring data within
an organisation is greatly reduced.

An intra-organisational synchronization engine greatly reduces the burden of data sharing between
teams. I believe that synchronization is the better abstraction when compared to either fetching or
event streaming.

## Sync extra use case

This would also bring automatic cache invalidation as benefit, or rather caches are simply updated
automatically.

## Engineers knowing everything

From these examples it should already be clear that cohesion between products is all about sharing
data. The actual burden that the software engineer experiences in this case is not knowing about
absolute everything within the organisation. This is by design, they should generally be focused on
their own product. Obviously, this is at odds with the idea of product cohesion.

## Use Cases

The necessity of data integration between products also shows up in the critical flow of
organisations. Consider a large financial institution offering products globally: two teams working
on completely separate automation projects, the first automating loans while the second is
automating credit cards. The teams might not even know about each others existence depending on the
size of the organisation. A single customer applying for both products might be a risk to the
financial institution, since both products increase the lending position of this single customer.
With a product-data catalog the teams could find relevant data and avoid the risks.

# What is cohesion

High product cohesion is when your customers can interact with multiple products without leaving the
product they initially opened. An example is an email client showing calendar data when the user
gets invited to an event. Another is being able to seamlessly copy data from their clipboard on
their phone to their computer. High cohesion binds users to your product by keeping them in your
ecosystem. A more technical example is when there is a single API endpoint and authentication
mechanism that is the gateway to all (API based) products.

# Data as a Burden

## Data Burden

To software engineers, data is a burden. Or more accurately, state is a burden. Stateless code is
easy to reason about because there are no external factors to worry about. Data however, is a
necessary evil. It's impossible to deliver value to users without "remembering" who they are and
what they were doing. This necessary evil slows down development, not only by making reasoning about
the program harder, and thus making the actual process of writing code slower. It also requires
"plumbing" code to move data from the data layer to the business logic.

## Misc data burden

Unfortunately this is just one burden that data places on software engineers. There is storage to
consider, and its faults of course. Luckily there are redundant setups to alleviate these problems.
There is caching, and more importantly cache invalidation to worry about. Fetching data and updating
are not solved problems either. Don't forget about concurrency issues when multiple clients are
changing the same piece of data. Notifying others about changes to our data is also something to
consider.

# Database Burden

Schemas and constraints are undoubtedly useful, but also the source of headaches. Application
requirements change, and so the schema has to be adapted. This is arguably one of the hardest things
in software engineering. If you're lucky, all the data you require is already in the database, and
simply has to change forms. In a less fortunate case you would have to perform a migration that
requires data from outside the store. In the worst case you might even have to move your data to a
completely new data store.

State is abstracted by database systems so the services we write may be stateless. Most databases
provide guardrails to improve the integrity of the data they store: schemas & constraints. This way
we know that our service is in a consistent state at the time of startup. After that moment, we lose
guarantees: buggy code may transform data in unexpected ways. Through the power of databases, we get
the advantage of being able to restart if something goes wrong.

# Why Care?

Clearly data is a double-edged sword. But why do we even care about data burdening software
engineers? Being burdened by man-made reasoning problems is basically the job description for
"Software Engineer", right? There is one simple reason: the more burden placed on an engineer, the
slower the project they're involved in progresses. Software needs to be maintainable to be able to
ship fast. Dealing with data is a part of that.

# Unburdening

We established that data is valuable, and we don't want to lose that value. Logically the only thing
that there is to do then, is to make data less of a burden on the software engineers that are
building products which generate data. So how do we do that? Lets deep dive into old and new ideas
that will make working with data within organizations easier.
